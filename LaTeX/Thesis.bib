Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@techreport{Wang,
abstract = {The ability to recognize and make inductive inferences based on relational similarity is fundamental to much of human higher cognition. However, relational similarity is not easily defined or measured, which makes it difficult to determine whether individual differences in cognitive capacity or semantic knowledge impact relational processing. In two experiments, we used a multi-arrangement task (previously applied to individual words or objects) to efficiently assess similarities between word pairs instantiating various abstract relations. Experiment 1 established that the method identifies word pairs expressing the same relation as more similar to each other than to those expressing different relations. Experiment 2 extended these results by showing that relational similarity measured by the multi-arrangement task is sensitive to more subtle distinctions. Word pairs instantiating the same specific subrelation were judged as more similar to each other than to those instantiating different subrelations within the same general relation type. In addition, Experiment 2 found that individual differences in both fluid intelligence and crystalized verbal intelligence correlated with differentiation of relation similarity judgments.},
author = {Ichien, Nicholas and Lu, Hongjing and Holyoak, Keith J},
file = {:H$\backslash$:/Downloads/RelationalSimilarity{\_}CogSci2019.pdf:pdf},
keywords = {crystallized intelligence,fluid intelligence,relational reasoning,semantic cognition,similarity},
title = {{Individual Differences in Judging Similarity Between Semantic Relations}},
url = {https://www.researchgate.net/publication/334119254{\_}Individual{\_}Differences{\_}in{\_}Judging{\_}Similarity{\_}Between{\_}Semantic{\_}Relations}
}
@techreport{Penningtona,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - Unknown - GloVe Global Vectors for Word Representation.pdf:pdf},
title = {{GloVe: Global Vectors for Word Representation}},
url = {http://nlp.}
}
@techreport{Ichien,
abstract = {The ability to recognize and make inductive inferences based on relational similarity is fundamental to much of human higher cognition. However, relational similarity is not easily defined or measured, which makes it difficult to determine whether individual differences in cognitive capacity or semantic knowledge impact relational processing. In two experiments, we used a multi-arrangement task (previously applied to individual words or objects) to efficiently assess similarities between word pairs instantiating various abstract relations. Experiment 1 established that the method identifies word pairs expressing the same relation as more similar to each other than to those expressing different relations. Experiment 2 extended these results by showing that relational similarity measured by the multi-arrangement task is sensitive to more subtle distinctions. Word pairs instantiating the same specific subrelation were judged as more similar to each other than to those instantiating different subrelations within the same general relation type. In addition, Experiment 2 found that individual differences in both fluid intelligence and crystalized verbal intelligence correlated with differentiation of relation similarity judgments.},
author = {Ichien, Nicholas and Lu, Hongjing and Holyoak, Keith J},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ichien, Lu, Holyoak - Unknown - Individual Differences in Judging Similarity Between Semantic Relations.pdf:pdf},
keywords = {crystallized intelligence,fluid intelligence,relational reasoning,semantic cognition,similarity},
title = {{Individual Differences in Judging Similarity Between Semantic Relations}}
}
@techreport{Pennington,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Socher, Manning - Unknown - GloVe Global Vectors for Word Representation(2).pdf:pdf},
title = {{GloVe: Global Vectors for Word Representation}},
url = {http://nlp.}
}
@techreport{Caliskan,
abstract = {Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language-the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model-namely, the GloVe word embedding-trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.},
archivePrefix = {arXiv},
arxivId = {1608.07187v4},
author = {Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
eprint = {1608.07187v4},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caliskan, Bryson, Narayanan - Unknown - Semantics derived automatically from language corpora necessarily contain human biases.pdf:pdf},
title = {{Semantics derived automatically from language corpora necessarily contain human biases}},
url = {http://opus.bath.ac.uk/55288/}
}
@article{Tshitoyan,
abstract = {The overwhelming majority of scientific knowledge is published as text, which is difficult to analyse by either traditional statistical analysis or modern machine learning methods. By contrast, the main source of machine-interpretable data for the materials research community has come from structured property databases 1,2 , which encompass only a small fraction of the knowledge present in the research literature. Beyond property values, publications contain valuable knowledge regarding the connections and relationships between data items as interpreted by the authors. To improve the identification and use of this knowledge, several studies have focused on the retrieval of information from scientific literature using supervised natural language processing 3-10 , which requires large hand-labelled datasets for training. Here we show that materials science knowledge present in the published literature can be efficiently encoded as information-dense word embeddings 11-13 (vector representations of words) without human labelling or supervision. Without any explicit insertion of chemical knowledge, these embeddings capture complex materials science concepts such as the underlying structure of the periodic table and structure-property relationships in materials. Furthermore, we demonstrate that an unsupervised method can recommend materials for functional applications several years before their discovery. This suggests that latent knowledge regarding future discoveries is to a large extent embedded in past publications. Our findings highlight the possibility of extracting knowledge and relationships from the massive body of scientific literature in a collective manner, and point towards a generalized approach to the mining of scientific literature. Assignment of high-dimensional vectors (embeddings) to words in a text corpus in a way that preserves their syntactic and semantic relationships is one of the most fundamental techniques in natural language processing (NLP). Word embeddings are usually constructed using machine learning algorithms such as GloVe 13 or Word2vec 11,12 , which use information about the co-occurrences of words in a text corpus. For example, when trained on a suitable body of text, such methods should produce a vector representing the word 'iron' that is closer by cosine distance to the vector for 'steel' than to the vector for 'organic'. To train the embeddings, we collected and processed approximately 3.3 million scientific abstracts published between 1922 and 2018 in more than 1,000 journals deemed likely to contain materials-related research, resulting in a vocabulary of approximately 500,000 words. We then applied the skip-gram variation of Word2vec, which is trained to predict context words that appear in the proximity of the target word as a means to learn the 200-dimensional embedding of that target word, to our text corpus (Fig. 1a). The key idea is that, because words with similar meanings often appear in similar contexts, the corresponding embeddings will also be similar. More details about the model are included in the Methods and in Supplementary Information sections S1 and S2, where we also discuss alternative algorithm options such as GloVe. We find that, even though no chemical information or interpretation is added to the algorithm, the obtained word embeddings behave consistently with chemical intuition when they are combined using various vector operations (projection, addition, subtraction). For example, many words in our corpus represent chemical compositions of materials, and the five materials most similar to LiCoO 2 (a well-known lithium-ion cathode compound) can be determined through a dot product (projection) of normalized word embeddings. According to our model, the compositions with the highest similarity to LiCoO 2 are LiMn 2 O 4 , LiNi 0.5 Mn 1.5 O 4 , LiNi 0.8 Co 0.2 O 2 , LiNi 0.8 Co 0.15 Al 0.05 O 2 and LiNiO 2-all of which are also lithium-ion cathode materials. Similar to the observation made in the original Word2vec paper 11 , these embeddings also support analogies, which in our case can be domain-specific. For instance, 'NiFe' is to 'ferromagnetic' as 'IrMn' is to '?' , where the most appropriate response is 'antiferromagnetic'. Such analogies are expressed and solved in the Word2vec model by finding the nearest word to the result of subtraction and addition operations between the embeddings. Hence, in our model, − + ≈ ferromagnetic NiFe IrMn antiferromagnetic To better visualize such embedded relationships, we projected the embeddings of Zr, Cr and Ni, as well as their corresponding oxides and crystal structures, onto two dimensions using principal component analysis (Fig. 1b). Even in reduced dimensions, there is a consistent operation in vector space for the concepts 'oxide of ' (Zr − ZrO 2 ≈ Cr − Cr 2 O 3 ≈ Ni − NiO) and 'structure of ' (Zr − HCP ≈ Cr − BCC ≈ Ni − FCC). This suggests that the positions of the embeddings in space encode materials science knowledge such as the fact that zirconium has a hexagonal close packed (HCP) crystal structure under standard conditions and that its principal oxide is ZrO 2. Other types of materials analogies captured by the model, such as functional applications and crystal symmetries, are listed in Extended Data Table 1. The accuracies for each category are close to 50{\%}-similar to the baseline set in the original Word2vec study 12. We stress that Word2vec treats these entities simply as strings, and no chemical interpretation is explicitly provided to the model; rather, materials knowledge is captured through the positions of the words in scientific abstracts. Notably, we also found that embeddings of chemical elements are representative of their positions in the periodic table when projected onto two dimensions (Extended Data Fig. 1a, b, Supplementary Information sections S4 and S5) and can serve as effective feature vectors in quantitative machine learning models such as formation energy prediction-outperforming several previously reported curated feature vectors (Extended Data Fig. 1c, d, Supplementary Information section S6). The main advantage and novelty of this representation, however, is that application keywords such as 'thermoelectric' have the same representation as material formulae such as 'Bi 2 Te 3 '. When the cosine similarity of a material embedding and the embedding of 'thermo-electric' is high, one might expect that the text corpus necessarily includes abstracts reporting on the thermoelectric behaviour of this material 14,15. However, we found that a number of materials that have relatively high cosine similarities to the word 'thermoelectric' never},
author = {Tshitoyan, Vahe and Dagdelen, John and Weston, Leigh and Dunn, Alexander and Rong, Ziqin and Kononova, Olga and Persson, Kristin A and Ceder, Gerbrand and Jain, Anubhav},
doi = {10.1038/s41586-019-1335-8},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tshitoyan et al. - Unknown - Unsupervised word embeddings capture latent knowledge from materials science literature.pdf:pdf},
journal = {Nature},
title = {{Unsupervised word embeddings capture latent knowledge from materials science literature}},
url = {https://doi.org/10.1038/s41586-019-1335-8}
}
@techreport{Mikolov,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781v3},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781v3},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://ronan.collobert.com/senna/}
}
@article{VidPodpecanNadaLavrac2018,
abstract = {This paper reports on the progress towards constructing auto- mated historiography of the research field of Computational Creativity (CC). The analysis is based on papers published in the Proceedings of International Conferences on Computa- tional Creativity in eight consecutive years since 2010. This paper extends our earlier work by proposing an approach to CC field analysis that facilitates the automation of CC con- ceptualisation.
},
author = {{Vid Podpecan Nada Lavra{\v{c}}}, Geraint Wiggins and Pollak, Senja},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vid Podpecan Nada Lavra{\v{c}}, Pollak - 2018 - Conceptualising Computational Creativity Towards automated historiography of a research field.pdf:pdf},
isbn = {978-989-51160-0-4},
journal = {Proceedings of the Ninth International Conference on Computational Creativity (ICCC'18)},
pages = {288--295},
title = {{Conceptualising Computational Creativity: Towards automated historiography of a research field}},
url = {http://computationalcreativity.net/iccc2018/sites/default/files/papers/ICCC{\_}2018{\_}paper{\_}66.pdf},
year = {2018}
}
@techreport{Ichien2019,
author = {Ichien, Nicholas and Lu, Hongjing and Holyoak, Keith J},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ichien, Lu, Holyoak - 2019 - Individual Differences in Judging Similarity Between Semantic Relations Human Perception of Social Interact.pdf:pdf},
title = {{Individual Differences in Judging Similarity Between Semantic Relations Human Perception of Social Interactions View project Exploring representation of relations through relational similarity View project}},
url = {https://www.researchgate.net/publication/334119254},
year = {2019}
}
@article{Ferraro2011,
abstract = {Research on the extraction of content relations from text corpora is a high-priority topic in natural language processing. This is not surprising since content relations form the backbone of any ontology, and ontologies are increasingly made use of in knowledge-based applications. However, so far most of the works focus on the detection of a restricted number of prominent verbal relations, including in particular is-a, has-part and cause. Our application, which aims to provide comprehensive, easy-to-understand content representations of complex functional objects described in patent claims, faces the need to derive a large number of content relations that cannot be limited a priori. To cope with this problem, we take advantage of the fact that deep syntactic dependency structures of sentences capture all relevant content relations - although without any abstraction. We implement thus a three-step strategy. First, we parse the claims to retrieve the deep syntactic dependency structures from which we then derive the content relations. Second, we generalize the obtained relations by clustering them according to semantic criteria, with the goal to unite all sufficiently similar relations. Finally, we identify a suitable name for each generalized relation. To keep the scope of the article within reasonable limits and to allow for a comparison with state-of-the-art techniques, we focus on verbal relations. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Ferraro, Gabriela and Wanner, Leo},
doi = {10.1016/j.knosys.2011.05.014},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferraro, Wanner - 2011 - Towards the derivation of verbal content relations from patent claims using deep syntactic structures.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Cluster labeling,Deep dependency parsing,Dependency relation,Relation clustering,Specialized discourse},
number = {8},
pages = {1233--1244},
publisher = {Elsevier B.V.},
title = {{Towards the derivation of verbal content relations from patent claims using deep syntactic structures}},
volume = {24},
year = {2011}
}
@inproceedings{Mikolova,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1310.4546},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
issn = {10495258},
title = {{Distributed representations ofwords and phrases and their compositionality}},
year = {2013}
}
@article{Trask2015,
abstract = {Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or "senses". Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8{\%} average error reduction in unlabeled attachment scores across 6 languages.},
archivePrefix = {arXiv},
arxivId = {1511.06388},
author = {Trask, Andrew and Michalak, Phil and Liu, John},
eprint = {1511.06388},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trask, Michalak, Liu - 2015 - sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings.pdf:pdf},
month = {nov},
title = {{sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings}},
url = {http://arxiv.org/abs/1511.06388},
year = {2015}
}
@article{Wang1,
abstract = {Vehicular ad hoc networks (VANETs) have emerged as a new powerful technology for data transmission between vehicles. Efficient data transmission accompanied with low data delay plays an important role in selecting the ideal data forwarding path in VANETs. This paper proposes a new opportunity routing protocol for data forwarding based on vehicle mobility association (OVMA).With assistance from the vehicle mobility association, data can be forwarded without passing through many extra intermediate nodes. Besides, each vehicle carries the only replica information to record its associated vehicle information, so the routing decision can adapt to the vehicle densities. Simulation results show that the OVMA protocol can extend the network lifetime, improve the performance of data delivery ratio, and reduce the data delay and routing overhead when compared to the other well-known routing protocols.},
author = {Wang, Leilei and Chen, Zhigang and Wu, Jia},
doi = {10.3390/info8040140},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Chen, Wu - 2017 - An Opportunistic Routing for Data Forwarding Based on Vehicle Mobility Association in Vehicular Ad Hoc Networks.pdf:pdf},
issn = {20782489},
journal = {Information (Switzerland)},
keywords = {Data forwarding,Opportunistic routing,VANETs,Vehicle association},
month = {nov},
number = {4},
pages = {140},
publisher = {MDPI AG},
title = {{An opportunistic routing for data forwarding based on vehicle mobility association in vehicular ad hoc networks}},
url = {http://www.mdpi.com/2078-2489/8/4/140},
volume = {8},
year = {2017}
}
@inproceedings{Qi,
abstract = {This paper describes Stanford's system at the CoNLL 2018 UD Shared Task. We introduce a complete neural pipeline system that takes raw text as input, and performs all tasks required by the shared task, ranging from tokenization and sentence segmentation, to POS tagging and dependency parsing. Our single system submission achieved very competitive performance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2nd, 1st, and 3rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on low-resource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies.},
archivePrefix = {arXiv},
arxivId = {1901.10457},
author = {Qi, Peng and Dozat, Timothy and Zhang, Yuhao and Manning, Christopher D},
booktitle = {CoNLL 2018 - SIGNLL Conference on Computational Natural Language Learning, Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
doi = {10.18653/v1/K18-2016},
eprint = {1901.10457},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qi et al. - Unknown - Universal Dependency Parsing from Scratch.pdf:pdf},
isbn = {9781948087827},
pages = {160--170},
title = {{Universal dependency parsing from scratch}},
url = {https://github.com/stanfordnlp/},
year = {2018}
}
@techreport{Schakel,
abstract = {Distributed representations of words as real-valued vectors in a relatively low-dimensional space aim at extracting syntactic and semantic features from large text corpora. A recently introduced neural network , named word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), was shown to encode semantic information in the direction of the word vectors. In this brief report, it is proposed to use the length of the vectors, together with the term frequency , as measure of word significance in a corpus. Experimental evidence using a domain-specific corpus of abstracts is presented to support this proposal. A useful visualization technique for text corpora emerges, where words are mapped onto a two-dimensional plane and automatically ranked by significance.},
archivePrefix = {arXiv},
arxivId = {1508.02297v1},
author = {Schakel, Adriaan M J and Wilson, Benjamin J},
eprint = {1508.02297v1},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schakel, Wilson - Unknown - Measuring Word Significance using Distributed Representations of Words.pdf:pdf},
title = {{Measuring Word Significance using Distributed Representations of Words}},
url = {http://groups.google.com/forum/{\#}!forum/word2vec-}
}
@article{Posadas,
abstract = {In this paper, we introduce an algorithm for obtaining the subtrees (continuous and non-continuous syntactic n-grams) from a dependency parse tree of a sentence. Our algorithm traverses the dependency tree of the sentences within a text document and extracts all its subtrees (syntactic n-grams). Syntactic n-grams are being successfully used in the literature (by ourselves and other authors) as features to characterize text documents using machine learning approach in the field of Natural Language Processing.},
author = {Posadas-Dur{\'{a}}n, Juan Pablo and Sidorov, Grigori and G{\'{o}}mez-Adorno, Helena and Batyrshin, Ildar and Mirasol-M{\'{e}}lendez, Elibeth and Posadas-Dur{\'{a}}n, Gabriela and Chanona-Hern{\'{a}}ndez, Liliana},
doi = {10.12700/APH.14.3.2017.3.5},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Posadas-Dur{\'{a}}n et al. - 2017 - Algorithm for extraction of subtrees of a sentence dependency parse tree.pdf:pdf},
issn = {17858860},
journal = {Acta Polytechnica Hungarica},
keywords = {Linguistic features,Subtrees extraction,Syntactic n-grams,Tree traversal},
number = {3},
pages = {79--98},
publisher = {Budapest Tech Polytechnical Institution},
title = {{Algorithm for extraction of subtrees of a sentence dependency parse tree}},
volume = {14},
year = {2017}
}
@article{Adams2015,
abstract = {Information professionals who train or instruct others can use Bloom's taxonomy to write learning objectives that describe the skills and abilities that they desire their learners to master and demonstrate. Bloom's taxonomy differentiates between cognitive skill levels and calls attention to learning objectives that require higher levels of cognitive skills and, therefore, lead to deeper learning and transfer of knowledge and skills to a greater variety of tasks and contexts.},
author = {Adams, Nancy E.},
doi = {10.3163/1536-5050.103.3.010},
file = {:C$\backslash$:/Users/Shane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adams - 2015 - Bloom's taxonomy of cognitive learning objectives.pdf:pdf},
issn = {15589439},
journal = {Journal of the Medical Library Association},
keywords = {Bloom's taxonomy: Learning objectives,Classification,Cognition,Teaching},
number = {3},
pages = {152--153},
pmid = {26213509},
publisher = {Medical Library Association},
title = {{Bloom's taxonomy of cognitive learning objectives}},
volume = {103},
year = {2015}
}
