The task of extracting context and meaning from language is a difficult task and one which requires a large quantity of input data. Previous projects such as Word2Vec have achieved the goal of obtaining some context when producing word embeddings. However this process avoids a part of language which is integral to a words meaning, part of speech is an often overlooked semantic of language. The Sense2Vec project proposes a solution to include part of speech tagging in the training process for producing word embeddings. Upon investigating the use of this tagging in word embeddings some major advantages however this process is not without its drawbacks.

The most noteworthy advantage of this process is the reduction of confusion when investigating the use of a word that may have multiple parts of speech. The ability to specify more meaning for a specific word not only improves the accuracy by removing unwanted word embeddings not relevant to the word used in this context, it also separates clusters that would otherwise be combined had they not be separated by part of speech. Within section 4.3 Collisions three examples of this scenario were presented with Word2Vec providing a combination of words related to both the noun and verb form of the word. When compared to tagging the word; Sense2Vec provided greater context and allowed for clear separation between both forms of the word in all three cases. It was possible to make a clear distinction between the two word embedding systems using the WordNet project and Bloom's Taxonomy as baselines.

This process does not come without some caveats however, with the separation of word embeddings based on their part of speech the resulting vocabulary size is larger despite technically containing the same number of unique words. This also introduces a loss in context as the surrounding words will now be linked to the specific form of the word in that context, based on whether it used as a noun, verb adjective etc. All this requires a larger corpus of text with greater a diversity of words used in different forms, compare this to Word2Vec which can obtain a reasonable level of context based on a smaller corpus. Of course Sense2Vec also requires the corpus to be part of speech tagged before the training process begins a lengthy process and the most complex problem to solve. However once this is complete the training process is significantly quicker and the model produced is over four times smaller than that of Word2Vec, this is helped by the use of the GloVe library used to produce the model. Appendix G contains further a comparison on training times and changes to the training process for future work.
