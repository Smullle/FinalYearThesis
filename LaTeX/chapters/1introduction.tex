\section{Natural Language Processing (NLP)}
The task of processing human language is one that is complex and is a combination of many areas of research including speech recognition, language generation and understanding to name a few. All developments in this area require the processing and analysis of large amounts of ordinary human language known as natural language. The area of NLP has importance many current technologies from obtaining personalised recommendations online to speech recognition to create text-to-speech systems. While it is clear in recent years huge developments have been made in this area and research in the field of natural language has been around for almost as long as language itself, the task of processing this natural language is still a non trivial problem. Firstly there is the problem of natural language itself, the first hurdle is syntax. This defines the formal grammar of the language, the part of speech of the words in a sentence example verb, noun adjective etc. Next is semantics which is based on the meaning of words in a sentence which is particularly important in translation, where it is important to identify the names of places and people in the English language these are capitalised other languages have different semantics for this. Both syntax and semantics both have more tasks to overcome to fully understand and describe the natural language they belong to. Chapter 2 presents a more in depth analysis of how these tasks are used in the area of natural language processing. 

With the sharp increase in the availability of data in recent years the possibility to develop more intelligent systems has drawn the attention of many researchers and companies. However the processing of this large amount of language data is now a new problem alongside the understanding of the language. While many methods have existed for years for the processing of this data, these methods were originally created before this abundance of language data was available and as such are not suited to processing the size now available. A gap was created as huge amounts of data were now available with few ways to process it, the introduction of machine learning was the plan to solve this issue. Many current solutions use this method, examples of such include the extraction of events for incoming emails, auto-correction of text and many more.

\section{Problem Overview}
The solution presented in chapter 3 will focus on the syntax and semantics of NLP, looking at the parts-of-speech in the English language and the similarity between them. While this similarity will investigate on all parts-of-speech there will be a focus on verbs and pronouns, as there appears to be little research done in this field. A new method for investigating this similarity between words has recently appeared, with this new method it is possible to compute the similarity between verbs and pronouns. This new method will be bench marked against older methods, the results of which will be presented in chapter 4, with an analysis of these comparisons presented in chapter 5.

Within chapter 4 a broad selection of results will be presented, these results will compare three technologies, Word2vec, Sense2Vec and WordNet. As discussed above there is now an abundance of data which can be analysed and used for modeling. In chapter 3 where this data was obtained and how it will be processed will be discussed. The training process will be shown in detail and improvements to this process will be presented. Any conclusions obtained from these results will show if any changes to previous technologies are positive, negative or unchanged. Not only is it important to highlight the positive and negatives of using these technologies, it is also key to validate the results produced and show a distinctive difference between the technologies. The methods used to validate the results and the technologies used will demonstrated at the end of chapter 2.